{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Models_a_tester.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wiIFvK2mhrUyb2mQpTh7UgdeyljsMwEZ",
      "authorship_tag": "ABX9TyMV3FHHCMaPI1skiGGhKRBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyeslehara1996/Matser_SI_DL/blob/main/Models_a_tester.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nflhM1NA1Psq"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam,Adamax,Adagrad,SGD,RMSprop\n",
        "\n",
        "from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda\n",
        "import keras.metrics as metrics\n",
        "from keras import backend as K\n",
        "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM,GRU, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import regularizers,constraints\n",
        "from keras import metrics\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "\n",
        "from keras import initializers as initializers\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import  keras\n",
        "import pandas as pd \n",
        "import re \n",
        "import nltk\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string as st\n",
        "SAVEd = False\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils.np_utils import to_categorical\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\"\"\" Dataset\"\"\"\n",
        "\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogIwWdqL1uLx"
      },
      "source": [
        "\n",
        "df=pd.read_excel(\"/content/drive/MyDrive/dossier_de_travail/SemEval2017A.xlsx\")\n",
        "\n",
        "df.drop(\"Unnamed: 3\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 4\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 5\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 6\", axis=1, inplace=True)\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYVUMrQ11y_b"
      },
      "source": [
        "\n",
        "#supprimer les lignes qui contient des valeur null \n",
        "df.Polarity.unique()\n",
        "df.dropna(subset=['Polarity'], inplace=True)\n",
        "df.Polarity.unique()\n",
        "df.info()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "df.Polarity.hist(xlabelsize=14)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJfbjF_y1y7n"
      },
      "source": [
        "#### transformet les mots en miniscule ######\n",
        "df.Comments=df.Comments.str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzL_B6es1y4A"
      },
      "source": [
        "\n",
        "###################STOP WORDS################\n",
        "#STOP WORDS\n",
        "#Tokenization of text\n",
        "\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "df['Comments']=df['Comments'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Juz4M2h01y0X"
      },
      "source": [
        "\n",
        "############supprission des caractere spiciaux Dans Commantaire #########\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'{link}', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'&[a-z]+;', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"[^a-z]\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'@mention', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()  if len(x)>3 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIefrx6R1ywZ"
      },
      "source": [
        "#######deviser en review and labels ######\n",
        "\n",
        "\n",
        "reviews =  df[['Comments']]\n",
        "labels =  df[['Polarity']]\n",
        "\n",
        "corpus= []\n",
        "for text in reviews['Comments']:\n",
        "    words= [word.lower() for word in word_tokenize(text)]\n",
        "    corpus.append(words)\n",
        "\n",
        "num_words=len(corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kdt9efeb2S63"
      },
      "source": [
        "####reviews sans ponctuation #######\n",
        "\n",
        "revue_sans_ponctuation=[]\n",
        "for sentence in reviews['Comments']:\n",
        "\n",
        "    revue_sans_ponctuation.append(' '.join(Word.strip(st.punctuation) for Word in sentence.split()))\n",
        "\n",
        "reviews_cleaned = np.asarray(revue_sans_ponctuation)\n",
        "reviews_cleaned\n",
        "\n",
        "\n",
        "\n",
        "review_array = np.asarray(revue_sans_ponctuation)\n",
        "label_array = np.asarray(labels['Polarity'])\n",
        "\n",
        "reviews_labels = np.stack((review_array, label_array), axis = 1)\n",
        "\n",
        "reviews_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADk6nAf92S4N"
      },
      "source": [
        "########Encoder les polarity  ##############\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(label_array)\n",
        "encoded_labels = encoder.transform(label_array)\n",
        "encoded_labels = to_categorical(encoded_labels)\n",
        "encoded_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnGlcf3z2S1H"
      },
      "source": [
        "##### Train and Test\n",
        "review_train, review_test, label_train, label_test = train_test_split(reviews_cleaned, encoded_labels, test_size=0.20, random_state=42)\n",
        "print(review_train.shape, label_train.shape)\n",
        "print(review_test.shape, label_test.shape)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "\n",
        "review_train = tokenizer.texts_to_sequences(review_train)\n",
        "review_test = tokenizer.texts_to_sequences(review_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwtVyikE2SyF"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/MyDrive/dossier_de_travail/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "    \n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        \n",
        "print(embedding_matrix[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N123nFPj2Sut"
      },
      "source": [
        "X_train=review_train  \n",
        "X_test=review_test\n",
        "y_train=label_train   \n",
        "y_test=label_test\n",
        "MAX_LENGTH = 100\n",
        "NUM_WORDS = vocab_size\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_LENGTH)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "train_index = np.random.choice(np.arange(X_train.shape[0]), 400, replace=False)     \n",
        "test_index = np.random.choice(np.arange(X_test.shape[0]), 250, replace=False)      \n",
        "\n",
        "X_train = X_train[train_index]\n",
        "y_train = y_train[train_index]\n",
        "\n",
        "X_test = X_test[test_index]\n",
        "y_test = y_test[test_index]\n",
        "print('X_train',X_train.shape)\n",
        "print('y_train',y_train.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('y_test',y_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba-JfuKD2qtT"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\t\n",
        "\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKO2Qq_f34YF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttvBTabMs3QB"
      },
      "source": [
        "# Utilisation de Grid_Search "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX6OYCbss_gx"
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def create_model(learning_rate=0.1,dropout=0.25):\n",
        "    Model_1 = Sequential()\n",
        "    Model_1.add(Embedding(vocab_size,100,weights=[embedding_matrix], input_length=100, trainable=False,name='word_embedding'))\n",
        "    Model_1.add( LSTM(850, dropout=dropout,return_sequences=True))\n",
        "    Model_1.add(Dropout(0.25))\n",
        "    Model_1.add( LSTM(500, dropout=dropout,return_sequences=False))\n",
        "    Model_1.add(Dense(3,activation='softmax'))\n",
        "    Model_1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "    Model_1.summary()\n",
        "    return Model_1  \n",
        "Model_1 = KerasClassifier(build_fn = create_model, batch_size=50, epochs=3)\n",
        "#now write out all the parameters you want to try out for the grid search\n",
        "learning_rate =np.array( [ 0.001,0.0001,0.000001])\n",
        "dropout=np.array([0.2,0.25])\n",
        "batch_size=np.array([80,100,128])\n",
        "param_grid = dict( learning_rate=learning_rate,dropout=dropout,batch_size=batch_size)\n",
        "grid = GridSearchCV(estimator=Model_1, param_grid=param_grid)\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsGMAmuJ3jCo"
      },
      "source": [
        "grid_result = grid.fit(X_train, y_train)\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llui0lPXs_d0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn8QDKvAs_Zv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o3bI5n5346G"
      },
      "source": [
        "# Model_1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjPi35x12Sro"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/ModelTest2'\n",
        "MODEL_FILE_NAME = 'Model1.h5'\n",
        "Model_1 = Sequential()\n",
        "Model_1.add(Embedding(vocab_size,100,weights=[embedding_matrix], input_length=100, trainable=False,name='word_embedding'))\n",
        "\n",
        "Model_1.add( LSTM(800, dropout=0.25,return_sequences=True))\n",
        "Model_1.add(Dropout(0.25))\n",
        "\n",
        "Model_1.add(LSTM(250, dropout=0.25,return_sequences=True))\n",
        "Model_1.add(Dropout(0.25))\n",
        "\n",
        "Model_1.add( LSTM(50, dropout=0.25,return_sequences=False))\n",
        "\n",
        "Model_1.add(Dense(3,activation='softmax'))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_accuracy', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "Model_1.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "\n",
        "Model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afUQ0Cns2qqH"
      },
      "source": [
        "history=Model_1.fit(X_train, y_train, batch_size=100, epochs=100, verbose=1, validation_data=(X_test,y_test),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4kkEDHM4Mnj"
      },
      "source": [
        "Model_1.evaluate(X_test,y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCdLl8Zr2qnB"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "plt.plot(history.history['f1_m'])\n",
        "plt.plot(history.history['val_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA0AZGbk7y4u"
      },
      "source": [
        "# Model_2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzjHCXKp7I6_"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/ModelTest2'\n",
        "MODEL_FILE_NAME = 'Mode_2.h5'\n",
        "Model_2 = Sequential()\n",
        "Model_2.add(Embedding(vocab_size,100,weights=[embedding_matrix], input_length=100, trainable=False,name='word_embedding'))\n",
        "\n",
        "Model_2.add( Bidirectional( LSTM(800, dropout=0.25,return_sequences=True)))\n",
        "Model_2.add(Dropout(0.25))\n",
        "\n",
        "Model_2.add(Bidirectional(LSTM(250, dropout=0.25,return_sequences=True)))\n",
        "Model_2.add(Dropout(0.25))\n",
        "\n",
        "Model_2.add(Bidirectional( LSTM(50, dropout=0.25,return_sequences=False)))\n",
        "\n",
        "Model_2.add(Dense(3,activation='softmax'))\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_accuracy', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "Model_2.compile(loss='binary_crossentropy',optimizer=Adam(learning_rate=0.0001),metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "\n",
        "Model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKIKvXsP9dYh"
      },
      "source": [
        "history=Model_2.fit(X_train, y_train, batch_size=100, epochs=100, verbose=1, validation_data=(X_test,y_test),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF10hrEY7fP4"
      },
      "source": [
        "Model_2.evaluate(X_test,y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL7iwFPr7e3i"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['f1_m'])\n",
        "plt.plot(history.history['val_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jwaQD2V8d9t"
      },
      "source": [
        "# Model Transformer avec Bahdanau attention "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTTCmMd8hJe"
      },
      "source": [
        "**Code de l'attention :**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S_iyqdm8TLh"
      },
      "source": [
        "\n",
        "LR = 0.0001 # Default for Adam is 0.001\n",
        "N_BATCH = 128\n",
        "N_EPOCHS = 20\n",
        "N_HEADS = 8\n",
        "DIM_HIDDEN = 300 # Dimensionality of the hidden state h_t outputted by the LSTM\n",
        "# Dimensionality of the weight matrices used while computing the attention, note that we divide this into n_head parts later\n",
        "DIM_KEY = N_HEADS*64\n",
        "\n",
        "\n",
        "# A more general and complete version of the layer defined in the linked keras example\n",
        "\n",
        "class MultiHeadSelfAttention(Layer):\n",
        "    \"\"\" This uses Bahadanau attention \"\"\"\n",
        "    \n",
        "    def __init__(self, num_heads = 8, weights_dim = 64):\n",
        "        \"\"\" Constructor: Initializes parameters of the Attention layer \"\"\"\n",
        "        \n",
        "        # Initialize base class:\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        \n",
        "        # Initialize parameters of the layer:\n",
        "        self.num_heads = num_heads\n",
        "        self.weights_dim = weights_dim\n",
        "        \n",
        "        if self.weights_dim % self.num_heads != 0:\n",
        "            raise ValueError(f\"Weights dimension = {weights_dim} should be divisible by number of heads = {num_heads} to ensure proper division into sub-matrices\")\n",
        "            \n",
        "        # We use this to divide the Q,K,V matrices into num_heads submatrices, to compute multi-headed attention\n",
        "        self.sub_matrix_dim = self.weights_dim // self.num_heads \n",
        "        \n",
        "       \n",
        "        \n",
        "        # Weight matrices for computing query, key and value (Note that we haven't defined an activation function anywhere)\n",
        "        # Important: In keras units contain the shape of the output\n",
        "        self.W_q = Dense(units = weights_dim) \n",
        "        self.W_k = Dense(units = weights_dim)\n",
        "        self.W_v = Dense(units = weights_dim)\n",
        "        \n",
        "        \n",
        "    def get_config(self):\n",
        "        \"\"\" Required for saving/loading the model \"\"\"\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            \"num_heads\" : self.num_heads,\n",
        "            \"weights_dim\" : self.weights_dim\n",
        "            # All args of __init__() must be included here\n",
        "        })\n",
        "        return config\n",
        "    \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        \"\"\" Initializes various weights dynamically based on input_shape \"\"\"\n",
        "        input_dim = input_shape[-1]\n",
        "        self.input_dim = input_dim\n",
        "        # Weight matrix for combining the output from multiple heads:\n",
        "        # Takes in input of shape (batch_size, seq_len, weights_dim) returns output of shape (batch_size, seq_len, input_dim)\n",
        "        self.W_h = Dense(units = input_dim)\n",
        "\n",
        "        \n",
        "    def attention(self, query, key, value):\n",
        "        \"\"\" The main logic \"\"\"\n",
        "        # Compute the raw score = QK^T \n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        \n",
        "        # Scale by dimension of K\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) # == DIM_KEY\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        \n",
        "        # Weights are the softmax of scaled scores\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        \n",
        "        # The final output of the attention layer (weighted sum of hidden states)\n",
        "        output = tf.matmul(weights, value)\n",
        "        \n",
        "        return output, weights\n",
        "\n",
        "    \n",
        "    def separate_heads(self, x, batch_size):\n",
        "        \"\"\" \n",
        "            Splits the given x into num_heads submatrices and returns the result as a concatenation of these sub-matrices\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.sub_matrix_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        \"\"\" All computations take place here \"\"\"\n",
        "        \n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        \n",
        "        # Compute Q = W_q*X\n",
        "        query = self.W_q(inputs)  # (batch_size, seq_len, weights_dim)\n",
        "        \n",
        "        # Compute K = W_k*X\n",
        "        key = self.W_k(inputs)  # (batch_size, seq_len, weights_dim)\n",
        "        \n",
        "        # Compute V = W_v*X\n",
        "        value = self.W_v(inputs)  # (batch_size, seq_len, weights_dim)\n",
        "        \n",
        "        # Split into n_heads submatrices\n",
        "        query = self.separate_heads(query, batch_size)  # (batch_size, num_heads, seq_len, sub_matrix_dim)\n",
        "        key = self.separate_heads(key, batch_size)  # (batch_size, num_heads, seq_len, sub_matrix_dim)\n",
        "        value = self.separate_heads(value, batch_size) # (batch_size, num_heads, seq_len, sub_matrix_dim)\n",
        "        \n",
        "        # Compute attention (contains weights and attentions for all heads):\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len, num_heads, sub_matrix_dim)\n",
        "        \n",
        "        # Concatenate all attentions from different heads (squeeze the last dimension):\n",
        "        concat_attention = tf.reshape(attention, (batch_size, -1, self.weights_dim))  # (batch_size, seq_len, weights_dim)\n",
        "        \n",
        "        # Use a weighted average of the attentions from different heads:\n",
        "        output = self.W_h(concat_attention)  # (batch_size, seq_len, input_dim)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        print(input_shape)\n",
        "        \"\"\" Specifies the output shape of the custom layer, without this, the model doesn't work \"\"\"\n",
        "        return input_shape\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cx4DXIFt7xFD"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/ModelTest2'\n",
        "MODEL_FILE_NAME = 'Mode_3.h5'\n",
        "\n",
        "def buildModel1():\n",
        "    \n",
        "    # The input as sequences:\n",
        "    input_layer = Input(shape=(MAX_LENGTH,))\n",
        "    # Create the embedding layer\n",
        "    embedding_layer = Embedding(\n",
        "        vocab_size,\n",
        "        100,\n",
        "        weights = [embedding_matrix],\n",
        "        input_length = MAX_LENGTH,\n",
        "        trainable = False # No need to train as our embeddings are already finetuned on Twitter data\n",
        "    )\n",
        "    # Create the embeddings\n",
        "    embedded_sequences = embedding_layer(input_layer)\n",
        "\n",
        "    # Single layer BiLSTM architecture\n",
        "    h = Bidirectional( LSTM(units = 800,  dropout = 0.25, return_sequences = True, ),  merge_mode = \"concat\"     )(embedded_sequences) \n",
        "    h = Bidirectional( LSTM(units = 300,  dropout = 0.25, return_sequences = True, ),  merge_mode = \"concat\" # Just like in Transformers, thus output h = [h_f; h_b] will have dimension 2*DIM_HIDDEN\n",
        "    )(h) \n",
        "\n",
        "    \n",
        "    # Adding multiheaded self attention\n",
        "    x = MultiHeadSelfAttention(N_HEADS, DIM_KEY)(h)\n",
        "    \n",
        "    outputs = Flatten()(x)\n",
        "    \n",
        "    model = Model(input_layer, outputs)\n",
        "    return model\n",
        "\n",
        "def buildModel2():\n",
        "    input_layer = Input(shape = (2*MAX_LENGTH*DIM_HIDDEN,))\n",
        "    # Fully connected classifer\n",
        "    x = Dense(1024, activation = \"relu\")(input_layer)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(256, activation = \"relu\")(x)\n",
        "    x = Dropout(0.25)(x)\n",
        "    x = Dense(32, activation = \"relu\")(x)\n",
        "    outputs = Dense(3, activation = \"softmax\")(x)\n",
        "    model = Model(input_layer, outputs)\n",
        "    return model\n",
        "\n",
        "def combineModels():\n",
        "    model1 = buildModel1()\n",
        "    model2 = buildModel2()\n",
        "    \n",
        "    input1 = Input(shape=(MAX_LENGTH,))\n",
        "    out1 = model1(input1)\n",
        "    out2 = model2(out1)\n",
        "    \n",
        "    model = Model(input1, out2)\n",
        "    return model\n",
        "\n",
        "#model = buildModel()\n",
        "model = combineModels()\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_accuracy', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "model.compile(\n",
        "    optimizer =Adam(learning_rate=0.0001), \n",
        "    loss = \"binary_crossentropy\", # As binary classification\n",
        "    metrics = ['accuracy',f1_m,precision_m, recall_m] # Balanced classes, thus good enough metric\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCMh6EtR7w7V"
      },
      "source": [
        "history=model.fit(X_train, y_train, batch_size=100, epochs=100, verbose=1, validation_data=(X_test,y_test),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av9oGM3d7wzs"
      },
      "source": [
        "model.evaluate(X_test,y_test,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RXgZ8067wvh"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['f1_m'])\n",
        "plt.plot(history.history['val_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCAtJSIS6fBx"
      },
      "source": [
        "# Model_4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZYspFt6o-E"
      },
      "source": [
        "code de Bi_LSTM avec self Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WIoxuMi6exK"
      },
      "source": [
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \n",
        "        self.supports_masking = True\n",
        "        #self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "       # self.w = self.add_weight(name='attention_weigth',shape=(input_shape[-1],1), initializer='normal')            \n",
        "       #self.b = self.add_weight(name='attention_bias',shape=(input_shape[-2],1), initializer='zero')\n",
        "        \n",
        "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
        "                                shape= (input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
        "                                      shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "        eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "        \n",
        "          a= K.cast(mask, K.floatx())\n",
        "\n",
        "        a= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "        \n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0],  self.features_dim\n",
        "\n",
        "    def get_config(self):\n",
        "\n",
        "        config = super().get_config().copy()\n",
        "        config.update({\n",
        "            'vocab_size':   self.W_regularizer,\n",
        "            'num_layers': self.step_dim,\n",
        "            'units': self.b_regularizer,\n",
        "            'd_model': self.W_constraint,\n",
        "            'num_heads': self.b_constraint,\n",
        "        })\n",
        "        return config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDuXCC066eoQ"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/ModelTest2'\n",
        "MODEL_FILE_NAME = 'Mode_4.h5'\n",
        "\n",
        "input_comment = Input(shape = (MAX_LENGTH,), dtype = 'int32')\n",
        "embedding_layer = Embedding(vocab_size, 100, weights = [embedding_matrix], input_length = MAX_LENGTH, trainable = False)\n",
        "\n",
        "embedded_sequence = embedding_layer(input_comment)\n",
        "x = Bidirectional(LSTM(800, dropout = 0.25, return_sequences = True)) (embedded_sequence)\n",
        "h = Bidirectional(LSTM(300, dropout = 0.25, return_sequences = True))(x)\n",
        "\n",
        "merged = Attention(MAX_LENGTH)(x)\n",
        "merged = Dense(1024, activation='relu')(merged)\n",
        "merged = Dropout(0.25)(merged)\n",
        "merged = Dense(64, activation='relu')(merged)\n",
        "merged = Dropout(0.25)(merged)\n",
        "merged = Dense(32, activation='relu')(merged)\n",
        "merged = Dropout(0.25)(merged)\n",
        "#merged = BatchNormalization()(merged)\n",
        "preds = Dense(3, activation='softmax')(merged)\n",
        "model = Model(inputs=[input_comment], outputs=preds)\n",
        "    \n",
        "    \n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_accuracy', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "        optimizer=Adam(learning_rate=0.0001),\n",
        "        metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train, \n",
        "    batch_size = 100, \n",
        "    epochs = 100,\n",
        "    validation_data = (X_test,y_test), \n",
        "    callbacks=[early_stopping, save_best]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puLKqf6k6edP"
      },
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['f1_m'])\n",
        "plt.plot(history.history['val_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9YLr5eK7wsV"
      },
      "source": [
        "model.evaluate(X_train,y_train,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}