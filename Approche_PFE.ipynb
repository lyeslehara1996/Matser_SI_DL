{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Approche-PFE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "W_FgrcJJsopo",
        "Psuxqcb3AznY",
        "LqY__iGGXHek",
        "zdogfwsE3Dv0",
        "Methws-GujOY",
        "TksoCTbW9WWu"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lyeslehara1996/Matser_SI_DL/blob/main/Approche_PFE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr1Pp1tocMmB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6072156-7f89-4757-cdc9-d8a8430a231c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC7CtfNOUZ8f"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam,Adamax,Adagrad,SGD,RMSprop\n",
        "import keras.metrics as metrics\n",
        "from keras import backend as K\n",
        "from keras.layers import Bidirectional, Dense, Embedding, Input, Lambda, LSTM,GRU, RepeatVector, TimeDistributed, Layer, Activation, Dropout\n",
        "from keras.models import Model\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import  keras\n",
        "import pandas as pd \n",
        "import re \n",
        "import nltk\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Dropout\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string as st\n",
        "SAVEd = False\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils.np_utils import to_categorical\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\"\"\" Dataset\"\"\"\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAnXWcAOqI8H"
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "# import tensorflow as tf\n",
        "from tensorflow.python.eager.context import eager_mode, graph_mode\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_flG53_pSu9"
      },
      "source": [
        "\n",
        "df=pd.read_excel(\"/content/drive/MyDrive/dossier_de_travail/SemEval2017A.xlsx\")\n",
        "\n",
        "df.drop(\"Unnamed: 3\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 4\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 5\", axis=1, inplace=True)\n",
        "df.drop(\"Unnamed: 6\", axis=1, inplace=True)\n",
        "\n",
        "df.head()\n",
        "\n",
        "\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KflpbespSxw"
      },
      "source": [
        "\n",
        "#supprimer les lignes qui contient des valeur null \n",
        "df.Polarity.unique()\n",
        "df.dropna(subset=['Polarity'], inplace=True)\n",
        "df.Polarity.unique()\n",
        "df.info()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "df.Polarity.hist(xlabelsize=14)\n",
        "plt.show()\n",
        "\n",
        "#### transformet les mots en miniscule ######\n",
        "df.Comments=df.Comments.str.lower()\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls1tMXYepS0-"
      },
      "source": [
        "\n",
        "###################STOP WORDS################\n",
        "#STOP WORDS\n",
        "#Tokenization of text\n",
        "\n",
        "tokenizer=ToktokTokenizer()\n",
        "#Setting English stopwords\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "#Apply function on review column\n",
        "df['Comments']=df['Comments'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbSVPqFmpS4A"
      },
      "source": [
        "\n",
        "############supprission des caractere spiciaux Dans Commantaire #########\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'{link}', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'&[a-z]+;', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r\"[^a-z]\", ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: re.sub(r'@mention', ' ', str(x)))\n",
        "df['Comments'] = df['Comments'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()  if len(x)>3 ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s7aQxXspS7E"
      },
      "source": [
        "#######deviser en review and labels ######\n",
        "\n",
        "\n",
        "reviews =  df[['Comments']]\n",
        "labels =  df[['Polarity']]\n",
        "\n",
        "corpus= []\n",
        "for text in reviews['Comments']:\n",
        "    words= [word.lower() for word in word_tokenize(text)]\n",
        "    corpus.append(words)\n",
        "\n",
        "num_words=len(corpus)\n",
        "print(num_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAg28SfMpS94"
      },
      "source": [
        "####reviews sans ponctuation #######\n",
        "\n",
        "revue_sans_ponctuation=[]\n",
        "for sentence in reviews['Comments']:\n",
        "\n",
        "    revue_sans_ponctuation.append(' '.join(Word.strip(st.punctuation) for Word in sentence.split()))\n",
        "\n",
        "reviews_cleaned = np.asarray(revue_sans_ponctuation)\n",
        "reviews_cleaned\n",
        "\n",
        "\n",
        "\n",
        "review_array = np.asarray(revue_sans_ponctuation)\n",
        "label_array = np.asarray(labels['Polarity'])\n",
        "\n",
        "reviews_labels = np.stack((review_array, label_array), axis = 1)\n",
        "\n",
        "reviews_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaBdFJ3LpTA6"
      },
      "source": [
        "########Encoder les polarity  ##############\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(label_array)\n",
        "encoded_labels = encoder.transform(label_array)\n",
        "encoded_labels = to_categorical(encoded_labels)\n",
        "encoded_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1HH_4IFpTDw"
      },
      "source": [
        "##### Train and Test\n",
        "review_train, review_test, label_train, label_test = train_test_split(reviews_cleaned, encoded_labels, test_size=0.20, random_state=42)\n",
        "print(review_train.shape, label_train.shape)\n",
        "print(review_test.shape, label_test.shape)\n",
        "\n",
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(review_train)\n",
        "\n",
        "review_train = tokenizer.texts_to_sequences(review_train)\n",
        "review_test = tokenizer.texts_to_sequences(review_test)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "355-5bwzpTGh"
      },
      "source": [
        "embeddings_dictionary = dict()\n",
        "glove_file = open('/content/drive/MyDrive/dossier_de_travail/glove.6B.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = asarray(records[1:], dtype='float32')\n",
        "    embeddings_dictionary [word] = vector_dimensions\n",
        "    \n",
        "glove_file.close()\n",
        "\n",
        "embedding_matrix = zeros((vocab_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_dictionary.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n",
        "        \n",
        "print(embedding_matrix[1].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lodVpVlOpTJr"
      },
      "source": [
        "X_train=review_train  \n",
        "X_test=review_test\n",
        "y_train=label_train   \n",
        "y_test=label_test\n",
        "MAX_LENGTH = 100\n",
        "NUM_WORDS = vocab_size\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=MAX_LENGTH)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "train_index = np.random.choice(np.arange(X_train.shape[0]), 400, replace=False)     \n",
        "test_index = np.random.choice(np.arange(X_test.shape[0]), 250, replace=False)      \n",
        "\n",
        "X_train = X_train[train_index]\n",
        "y_train = y_train[train_index]\n",
        "\n",
        "X_test = X_test[test_index]\n",
        "y_test = y_test[test_index]\n",
        "\n",
        "temp = np.zeros((X_train.shape[0], MAX_LENGTH, NUM_WORDS))\n",
        "temp[np.expand_dims(np.arange(X_train.shape[0]), axis=0).reshape(X_train.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_train.shape[0], axis=0), X_train] = 1\n",
        "\n",
        "Conversion_X_train = temp\n",
        "\n",
        "temp = np.zeros((X_test.shape[0], MAX_LENGTH, NUM_WORDS))\n",
        "temp[np.expand_dims(np.arange(X_test.shape[0]), axis=0).reshape(X_test.shape[0], 1), np.repeat(np.array([np.arange(MAX_LENGTH)]), X_test.shape[0], axis=0), X_test] = 1\n",
        "\n",
        "Conversion_X_test = temp\n",
        "print('X_train',X_train.shape)\n",
        "print('y_train',y_train.shape)\n",
        "print('X_test',X_test.shape)\n",
        "print('y_test',y_test.shape)\n",
        "print(Conversion_X_train.shape)\n",
        "print(Conversion_X_test.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72ODEdgrpTMV"
      },
      "source": [
        "\n",
        "from keras.layers import Flatten, Activation, RepeatVector, Permute, Multiply, Lambda\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvnIyhrxpTOn"
      },
      "source": [
        "\n",
        "class AttentionCouche(tf.keras.layers.Layer): \n",
        "    def __init__(self, **kwargs):   \n",
        "        self.dim_mots = 1600\n",
        "        self.unite_par_seq =100 \n",
        "\n",
        "        super(AttentionCouche, self).__init__( **kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "\n",
        "        dim_mots = input_shape[-1] \n",
        "        unite_par_seq = input_shape[-2]\n",
        "        num_units = 1\n",
        "        \n",
        "        self.w = self.add_weight(name='attention_weigth',shape=(input_shape[-1],1), initializer='normal')            \n",
        "        self.b = self.add_weight(name='attention_bias',shape=(input_shape[-2],1), initializer='zero')\n",
        "       \n",
        "        \n",
        "        super(AttentionCouche, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "       e=K.tanh(K.dot(x,self.w)+self.b) \n",
        "       e = Flatten()(e) \n",
        "       a = Activation('softmax')(e)\n",
        "       temp = RepeatVector(1600)(a) \n",
        "       temp = Permute([2,1])(temp) \n",
        "       output = Multiply()([x,temp]) \n",
        "       output = Lambda(lambda values: K.sum(values, axis=1))(output)\n",
        "       return output\n",
        "    def output_shep(self,input_shape):\n",
        "      return (input_shape[0],input_shape[-1])\n",
        "    def get_config(self):\n",
        "      return super(AttentionCouche,self).get_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSTqtfAwnGou"
      },
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
        "\t\n",
        "\t\n",
        "  #metrics=['acc',f1_m,precision_m, recall_m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_FgrcJJsopo"
      },
      "source": [
        "# Test_0\n",
        "Une Couche a 150 Neurones "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrYLlBpJsxvl"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test0.h5'\n",
        "LR=0.0001\n",
        "\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(150, return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    \n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    \n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    \n",
        "    return (fct_loss, z)\n",
        "     \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "    \n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = Bidirectional( LSTM(150, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXxdaiiUsxuE"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPtoQox6sxrc"
      },
      "source": [
        "autoencoder.evaluate(x=X_test, y={'decoded_mean': Conversion_X_test, 'pred': y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdJj35dssxp3"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKhoKAlmsxlw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Psuxqcb3AznY"
      },
      "source": [
        "# **Test_1** \n",
        "Une Couche a 250 Neurones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bv5bAyEf13Po"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQiLDdJwAzVB"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test1.h5'\n",
        "LR=0.0001\n",
        "\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(250, return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    \n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    \n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    \n",
        "    return (fct_loss, z)\n",
        "     \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "    \n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h =Bidirectional(  LSTM(250, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io5XuiNsLPO0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aB-H1oAA4D8"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMMXeg9jXDq7"
      },
      "source": [
        "autoencoder.evaluate(x=X_test, y={'decoded_mean': Conversion_X_test, 'pred': y_test})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sia0OhD4A4G_"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqY__iGGXHek"
      },
      "source": [
        "# Test 2:\n",
        "avec une couche cache avec 500 Neurones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ8_q7uIXHAg"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_2.h5'\n",
        "LR=0.0001\n",
        "\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(500, return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    \n",
        "    return (fct_loss, z)\n",
        "     \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h =Bidirectional(LSTM(500, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7WPqcLT-nTF"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSFRPMjj-nlr"
      },
      "source": [
        "autoencoder.evaluate(x=X_test, y={'decoded_mean': Conversion_X_test, 'pred': y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkTJxpOg-nWs"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdogfwsE3Dv0"
      },
      "source": [
        "# **Test_3**:\n",
        " Une couche avec 750 Neurones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i89XVgk3Dce"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_3.h5'\n",
        "LR=0.0001\n",
        "\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(750, return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    \n",
        "    return (fct_loss, z)\n",
        "     \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h =Bidirectional(LSTM(750, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ee0W4jxx3DW8"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnFxHvS73DPW"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Methws-GujOY"
      },
      "source": [
        "# Test_4\n",
        "Une Couche avec 800 unite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s71dhpYhj8-"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_4.h5'\n",
        "LR=0.0001\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(800,return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    return (fct_loss, z) \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "\n",
        "\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = Bidirectional(  LSTM(800, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=LR), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qf3Cfq3ihkLw"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmvlk_d7hkSz"
      },
      "source": [
        "autoencoder.evaluate(x=X_test, y={'decoded_mean': Conversion_X_test, 'pred': y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgrVELAShkPa"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX8fP4OEjtOf"
      },
      "source": [
        "test 5 avec 1024 unite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMvUh78mjyBA"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_5.h5'\n",
        "LR=0.0001\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(1024,return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    return (fct_loss, z) \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = Bidirectional(  LSTM(1024, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=LR), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeV9vL5Wjxv6"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysLNEN_Ujxua"
      },
      "source": [
        "autoencoder.evaluate(x=X_test, y={'decoded_mean': Conversion_X_test, 'pred': y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_DbGfipjxqc"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r25nJH0sjwTH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TksoCTbW9WWu"
      },
      "source": [
        "# **Test_5** :\n",
        "pleusieurs couches denses\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AinoILKnqPdX"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_4.h5'\n",
        "LR=0.0001\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(500,return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = Bidirectional(LSTM(500,return_sequences=True,dropout=0.25),merge_mode='concat')(h)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    return (fct_loss, z) \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = Bidirectional(  LSTM(500, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    h = Bidirectional(  LSTM(500, return_sequences=True, name='dec_lstm_1',dropout=0.25))(h)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=LR), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF1crKtAqPgL"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBr2iluWqPli"
      },
      "source": [
        "autoencoder.evaluate(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lbXtCUDqPtT"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M55BnTHu2xj"
      },
      "source": [
        "**Test_6**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL3skTrou1zS"
      },
      "source": [
        "MODEL_FILE_PATH = '/content/drive/MyDrive/Models_a_ameliorer'\n",
        "MODEL_FILE_NAME = 'Test_4.h5'\n",
        "LR=0.0001\n",
        "batch_size = 100\n",
        "vocab_size=vocab_size\n",
        "max_length=100 \n",
        "dim_latent=200\n",
        "intermediate_dim=500\n",
        "encoder = None\n",
        "decoder = None\n",
        "predicter_opinion = None\n",
        "autoencoder = None\n",
        "\n",
        "x = Input(shape=(max_length,))\n",
        "x_embed = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length)(x)\n",
        "def Encodeur(x, dim_latent=200, max_length=100, epsilon_std=0.01):\n",
        "    h = Bidirectional(LSTM(500,return_sequences=True,dropout=0.25),merge_mode='concat')(x)\n",
        "    h = Bidirectional(LSTM(250,return_sequences=True,dropout=0.25),merge_mode='concat')(h)\n",
        "    h = Bidirectional(LSTM(50,return_sequences=True,dropout=0.25),merge_mode='concat')(h)\n",
        "    h = AttentionCouche()(h) \n",
        "    h = Dense(300, activation='relu', name='dense_1')(h)\n",
        "    def sampling(args):\n",
        "        z_mean, z_log_var = args\n",
        "        batch = K.shape(z_mean)[0]\n",
        "        # by default, random_normal has mean=0 and std=1.0\n",
        "        epsilon = K.random_normal(shape=(batch, dim_latent), mean=0., stddev=epsilon_std)\n",
        "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "    z_mean = Dense(dim_latent, name='z_mean', activation='linear')(h)\n",
        "    z_log_var = Dense(dim_latent, name='z_log_var', activation='linear')(h)\n",
        "    z = Lambda(sampling, output_shape=(dim_latent,), name='z')([z_mean, z_log_var])\n",
        "    def fct_loss(x, x_decoded_mean):\n",
        "        x = K.flatten(x)\n",
        "        x_decoded_mean = K.flatten(x_decoded_mean)\n",
        "        xent_loss = max_length * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return xent_loss + kl_loss\n",
        "    return (fct_loss, z) \n",
        "fct_loss, encoded = Encodeur(x_embed, dim_latent=dim_latent, max_length=max_length)\n",
        "encoder = Model(inputs=x, outputs=encoded)\n",
        "encoder.summary()\n",
        "def Codage_opinion(encoded):\n",
        "    h = Dense(100, activation='linear')(encoded)     \n",
        "    return Dense(3, activation='softmax', name='pred')(h)\n",
        "\n",
        "encoded_input = Input(shape=(dim_latent,))\n",
        "opinion_lab = Codage_opinion(encoded_input)\n",
        "predicter_opinion = Model(encoded_input, opinion_lab)\n",
        "predicter_opinion.summary()\n",
        "def Decodeur(encoded, vocab_size, max_length):\n",
        "    repeated_context = RepeatVector(max_length)(encoded)\n",
        "    h = Bidirectional(  LSTM(500, return_sequences=True, name='dec_lstm_1',dropout=0.25))(repeated_context)\n",
        "    h = Bidirectional(  LSTM(250, return_sequences=True, name='dec_lstm_1',dropout=0.25))(h)\n",
        "    h = Bidirectional(  LSTM(50, return_sequences=True, name='dec_lstm_1',dropout=0.25))(h)\n",
        "    decoded = TimeDistributed(Dense(vocab_size, activation='softmax'), name='decoded_mean')(h)        \n",
        "    return decoded\n",
        "\n",
        "\n",
        "decoded = Decodeur(encoded_input, vocab_size, max_length)\n",
        "decoder = Model(encoded_input, decoded)\n",
        "decoder.summary()\n",
        "autoencoder = Model(inputs=x, outputs=[Decodeur(encoded, vocab_size, max_length), Codage_opinion(encoded)])\n",
        "tf.config.run_functions_eagerly(False)\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "save_best = ModelCheckpoint(os.path.join(MODEL_FILE_PATH,MODEL_FILE_NAME), \n",
        "                     monitor='val_pred_acc', \n",
        "                     mode='max', \n",
        "                     verbose=1, \n",
        "                     save_best_only=True) \n",
        "\n",
        "autoencoder.compile(optimizer=Adam(learning_rate=LR), loss=[fct_loss, 'binary_crossentropy'], metrics=['accuracy',f1_m,precision_m, recall_m])\n",
        "autoencoder.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXVI8nn_vc2U"
      },
      "source": [
        "\n",
        "history = autoencoder.fit(x=X_train, y={'decoded_mean': Conversion_X_train, 'pred': y_train}, batch_size=100, epochs=100, validation_data=(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test}),callbacks=[early_stopping, save_best])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G_46mbau7qg"
      },
      "source": [
        "autoencoder.evaluate(X_test, {'decoded_mean': Conversion_X_test, 'pred':  y_test})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvwBV-Iju7mk"
      },
      "source": [
        "plt.plot(history.history['pred_acc'])\n",
        "plt.plot(history.history['val_pred_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "# Plot of loss in each epoch\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "plt.plot(history.history['pred_loss'])\n",
        "plt.plot(history.history['val_pred_loss'])\n",
        "plt.title('model pred_loss')\n",
        "plt.ylabel('pred_loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['pred_f1_m'])\n",
        "plt.plot(history.history['val_pred_f1_m'])\n",
        "\n",
        "plt.title('model F1_score')\n",
        "plt.ylabel('f1_m')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train','test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE4egNNKu7is"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}